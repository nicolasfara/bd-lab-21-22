{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd7243a-b7d3-4e47-b3ec-49cafdebada5",
   "metadata": {},
   "source": [
    "# 301 Spark basics\n",
    "\n",
    "The goal of this lab is to get familiar with Spark programming.\n",
    "\n",
    "- [Spark programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "- [RDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html)\n",
    "- [PairRDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html)\n",
    "\n",
    "## 301-2 Running a sample Spark job\n",
    "\n",
    "Goal: calculate the average temperature for every month; dataset is ```weather-sample1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9323446c-bdfe-4eff-b87a-ce57f7c840b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-08T21:12:54.406362Z",
     "iopub.status.busy": "2022-03-08T21:12:54.406133Z",
     "iopub.status.idle": "2022-03-08T21:12:55.200385Z",
     "shell.execute_reply": "2022-03-08T21:12:55.199671Z",
     "shell.execute_reply.started": "2022-03-08T21:12:54.406339Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a71899180784d93975ff36edeeb1601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketname: String = unibo-bd2122-egallinucci\n",
      "rddWeather: org.apache.spark.rdd.RDD[String] = s3a://unibo-bd2122-egallinucci/datasets/weather-sample1.txt MapPartitionsRDD[48] at textFile at <console>:28\n"
     ]
    }
   ],
   "source": [
    "val bucketname = \"unibo-bd2122-egallinucci\"\n",
    "\n",
    "val rddWeather = sc.textFile(\"s3a://\"+bucketname+\"/datasets/weather-sample1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a5830c-1cd8-4f3a-bfc5-f91e9aa8d87f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-08T20:57:01.236989Z",
     "iopub.status.busy": "2022-03-08T20:57:01.236828Z",
     "iopub.status.idle": "2022-03-08T20:57:10.637785Z",
     "shell.execute_reply": "2022-03-08T20:57:10.637051Z",
     "shell.execute_reply.started": "2022-03-08T20:57:01.236968Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da7cea017e2470d98793012b5f5eb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parseWeatherLine: (line: String)(String, Double)\n",
      "rddWeatherKv: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[2] at map at <console>:29\n",
      "rddTempDataPerMonth: org.apache.spark.rdd.RDD[(String, (Double, Double))] = ShuffledRDD[3] at aggregateByKey at <console>:26\n",
      "rddAvgTempPerMonth: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[4] at map at <console>:26\n",
      "rddCached: org.apache.spark.rdd.RDD[(String, Double)] = CoalescedRDD[8] at coalesce at <console>:26\n"
     ]
    }
   ],
   "source": [
    "def parseWeatherLine(line:String):(String,Double) = {\n",
    "  val year = line.substring(15,19)\n",
    "  val month = line.substring(19,21)\n",
    "  val day = line.substring(21,23)\n",
    "  var temp = line.substring(87,92).toInt\n",
    "  (month, temp/10)\n",
    "}\n",
    "\n",
    "// Parse records\n",
    "val rddWeatherKv = rddWeather.map(x => parseWeatherLine(x))\n",
    "// Aggregate by key (i.e., month) to compute the sum and the count of temperature values\n",
    "val rddTempDataPerMonth = rddWeatherKv.aggregateByKey((0.0,0.0))((a,v)=>(a._1+v,a._2+1), (a1,a2)=>(a1._1+a2._1,a1._2+a2._2))\n",
    "// Calculate the average temperature in each record\n",
    "val rddAvgTempPerMonth = rddTempDataPerMonth.map({case(k,v) => (k, v._1/v._2)})\n",
    "// Sort, coalesce and cache the result (because it is used twice)\n",
    "val rddCached = rddAvgTempPerMonth.sortByKey().coalesce(1).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9621e29d-4c97-4779-b7d1-e5126bce5d42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-08T20:57:18.589619Z",
     "iopub.status.busy": "2022-03-08T20:57:18.589304Z",
     "iopub.status.idle": "2022-03-08T20:57:19.906211Z",
     "shell.execute_reply": "2022-03-08T20:57:19.905634Z",
     "shell.execute_reply.started": "2022-03-08T20:57:18.589590Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cea56a7e0524caeadea2c6e2bdfc770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res7: Array[(String, Double)] = Array((01,29.764781644286497), (02,52.831468961278425), (03,49.43499927074724), (04,61.3592872169286), (05,55.82656), (06,55.45816479125297), (07,86.90952392350223), (08,79.250958082407), (09,80.51662117371808), (10,106.26454490168254), (11,113.49704495968224), (12,63.9184413544602))\n"
     ]
    }
   ],
   "source": [
    "// Show all the records\n",
    "rddCached.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b50d2419-c6a0-4616-be1c-c24f09653107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-08T20:57:36.926333Z",
     "iopub.status.busy": "2022-03-08T20:57:36.926091Z",
     "iopub.status.idle": "2022-03-08T20:57:37.726128Z",
     "shell.execute_reply": "2022-03-08T20:57:37.725548Z",
     "shell.execute_reply.started": "2022-03-08T20:57:36.926306Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1495bd4200f5474eb57a2b3b5b79a45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory s3a://eg-myfirstbucket/spark/301-2 already exists\n",
      "  at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n",
      "  at org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:298)\n",
      "  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n",
      "  at org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\n",
      "  at org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\n",
      "  ... 51 elided\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddCached.saveAsTextFile(\"s3a://\"+bucketname+\"/spark/301-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05b8fdd-ef59-4e64-b749-1d9ae50260bc",
   "metadata": {},
   "source": [
    "## 301-3 Spark warm-up\n",
    "\n",
    "Load the ```capra``` and ```divinacommedia``` datasets and try the following actions:\n",
    "- Show their content (```collect```)\n",
    "- Count their rows (```count```)\n",
    "- Split phrases into words (```map``` or ```flatMap```; what’s the difference?)\n",
    "- Check the results (remember: evaluation is lazy)\n",
    "- Try the ```toDebugString``` function to check the execution plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52649088-9a57-437b-9127-0d29984b4e92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-08T21:03:12.571335Z",
     "iopub.status.busy": "2022-03-08T21:03:12.571109Z",
     "iopub.status.idle": "2022-03-08T21:03:13.362371Z",
     "shell.execute_reply": "2022-03-08T21:03:13.361537Z",
     "shell.execute_reply.started": "2022-03-08T21:03:12.571312Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e88b0a606bd4f2dbcc3602e39676eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rddCapra: org.apache.spark.rdd.RDD[String] = s3a://eg-myfirstbucket/first-datasets/capra.txt MapPartitionsRDD[21] at textFile at <console>:25\n"
     ]
    }
   ],
   "source": [
    "val rddCapra = sc.textFile(\"s3a://\"+bucketname+\"/datasets/capra.txt\")\n",
    "val rddDC = sc.textFile(\"s3a://\"+bucketname+\"/datasets/divinacommedia.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d9363-3ce0-411b-a826-ee3cceb935a1",
   "metadata": {},
   "source": [
    "## 301-4 From MapReduce to Spark\n",
    "\n",
    "Reproduce on Spark the exercises seen on Hadoop MapReduce on the capra and divinacommedia datasets.\n",
    "\n",
    "- Jobs:\n",
    "  - Count the number of occurrences of each word\n",
    "    - Result: (sopra, 1), (la, 4), …\n",
    "  - Count the number of occurrences of words of given lengths\n",
    "    - Result: (2, 4), (5, 8)\n",
    "  - Count the average length of words given their first letter (hint: check the example in 301-1)\n",
    "    - Result: (s, 5), (l, 2), …\n",
    "  - Return the inverted index of words\n",
    "    - Result: (sopra, (0)), (la, (0, 1)), ...\n",
    "- How does Spark compare with respect to MapReduce? (performance, ease of use)\n",
    "- How is the output sorted? How can you sort by value?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
