{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac4d70a-e105-4bb4-a1a8-129f410f8836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T15:34:09.444324Z",
     "iopub.status.busy": "2022-02-28T15:34:09.441785Z",
     "iopub.status.idle": "2022-02-28T15:34:10.302913Z",
     "shell.execute_reply": "2022-02-28T15:34:10.302209Z",
     "shell.execute_reply.started": "2022-02-28T15:34:09.444282Z"
    }
   },
   "source": [
    "# 304 Spark SQL\n",
    "\n",
    "The goal of this lab is to run some SQL queries.\n",
    "\n",
    "- [Spark SQL programming guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "\n",
    "This lab's notebook is in the ```material``` folder; the solutions will be released in the same folder.\n",
    "\n",
    "The cluster configuration should be the same from 301, 302, and 303."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a78c104-6d6f-48e6-b5ea-5880254f5cb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"executorMemory\":\"8G\", \"numExecutors\":2, \"executorCores\":2, \"conf\": {\"spark.dynamicAllocation.enabled\": \"false\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297e3f5-17d3-44ba-a06c-8b1acf0ca078",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "//val bucketname = \"unibo-bd2122-egallinucci\"\n",
    "val bucketname = \"eg-myfirstbucket\"\n",
    "\n",
    "val path_ml_movies = \"s3a://\"+bucketname+\"/datasets/ml-movies.csv\"\n",
    "val path_ml_ratings = \"s3a://\"+bucketname+\"/datasets/ml-ratings.csv\"\n",
    "val path_ml_tags = \"s3a://\"+bucketname+\"/datasets/ml-tags.csv\"\n",
    "\n",
    "sc.applicationId\n",
    "\n",
    "\"SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/\" + sc.applicationId + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e643e27d-b710-43cb-bc3d-7bca65e93b15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import java.util.Calendar\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "object MovieLensParser {\n",
    "\n",
    "  val noGenresListed = \"(no genres listed)\"\n",
    "  val commaRegex = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "  val pipeRegex = \"\\\\|(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "  val quotes = \"\\\"\"\n",
    "  \n",
    "  /** Convert from timestamp (String) to year (Int) */\n",
    "  def yearFromTimestamp(timestamp: String): Int = {\n",
    "    val cal = Calendar.getInstance()\n",
    "    cal.setTimeInMillis(timestamp.trim.toLong * 1000L)\n",
    "    cal.get(Calendar.YEAR)\n",
    "  }\n",
    "\n",
    "  /** Function to parse movie records\n",
    "   *\n",
    "   *  @param line line that has to be parsed\n",
    "   *  @return tuple containing movieId, title and genres, none in case of input errors\n",
    "   */\n",
    "  def parseMovieLine(line: String): Option[(Long, String, String)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "      var title = input(1).trim\n",
    "      title = if(title.startsWith(quotes)) title.substring(1) else title\n",
    "      title = if(title.endsWith(quotes)) title.substring(0, title.length - 1) else title\n",
    "      Some(input(0).trim.toLong, title, input(2).trim)\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /** Function to parse rating records\n",
    "   *\n",
    "   *  @param line line that has to be parsed\n",
    "   *  @return tuple containing userId, movieId, rating, and year none in case of input errors\n",
    "   */\n",
    "  def parseRatingLine(line: String): Option[(Long, Long, Double, Int)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "      Some(input(0).trim.toLong, input(1).trim.toLong, input(2).trim.toDouble, yearFromTimestamp(input(3)))\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /** Function to parse tag records\n",
    "   *\n",
    "   *  @param line line that has to be parsed\n",
    "   *  @return tuple containing userId, movieId, tag, and year, none in case of input errors\n",
    "   */\n",
    "  def parseTagLine(line: String) : Option[(Long, Long, String, Int)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "      Some(input(0).trim.toLong, input(1).trim.toLong, input(2), yearFromTimestamp(input(3)))\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e69ae6f-50f6-4e2f-9fe8-ff6d747e675f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val rddMovies = sc.textFile(path_ml_movies).flatMap(MovieLensParser.parseMovieLine)\n",
    "val rddRatings = sc.textFile(path_ml_ratings).flatMap(MovieLensParser.parseRatingLine)\n",
    "val rddTags = sc.textFile(path_ml_tags).flatMap(MovieLensParser.parseTagLine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f7132a-381b-4218-a261-2a6f9d2b6388",
   "metadata": {},
   "source": [
    "### 304-1 SQL querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb3d31b-7915-4620-a947-751d80cf7e89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rddMovies.toDF(\"movieId\",\"title\",\"genres\").createOrReplaceTempView(\"movies\")\n",
    "rddRatings.toDF(\"userid\",\"movieId\",\"rating\",\"year\").createOrReplaceTempView(\"ratings\")\n",
    "rddTags.toDF(\"userId\",\"movieId\",\"tag\",\"year\").createOrReplaceTempView(\"tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcd11ad-7cd0-405c-bdad-d38b53dc6f4d",
   "metadata": {},
   "source": [
    "Reduce the ```spark.sql.autoBroadcastJoinThreshold``` parameter, which determines the maximum size for DataFrames to be broadcasted (default value: \"10485760b\", i.e., 10MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2058912c-6f48-4409-955d-d1ffeb1e5bda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",\"1485760b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca34e2e9-0279-4ec6-95a4-524c8c2a17ce",
   "metadata": {},
   "source": [
    "Calculate the average rating for each movie with an SQL query. Check the results AND the execution plan.\n",
    "\n",
    "- Do you reckon some optimization by Catalyst?\n",
    "- Is there something more that could be done (besides broadcasting, that we have disabled)?\n",
    "\n",
    "Beware: Spark's UI is more difficult to read with SQL queries. Query execution times can be checked in the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749bb166-1e0a-46e0-bd07-a33b59387d63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val sqlDF = spark.sql(\"SELECT m.title, avg(r.rating), count(*) FROM movies m, ratings r WHERE m.movieId=r.movieId GROUP BY m.title\")\n",
    "sqlDF.explain\n",
    "sqlDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed8754c-b81d-46c2-9b70-1666c399d8a3",
   "metadata": {},
   "source": [
    "Catalyst carries out column pruning (see execution plan), but does not pushdown the aggregation. Why?\n",
    "1. It doesn't know that \"title\" is a descriptive attribute of movieid\n",
    "2. It doesn't know that we are assuming a unique title for each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f04105c-e0c7-4641-9f1e-48dd6b378fa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val sqlDF2 = spark.sql(\"SELECT movieId, avg(rating) as avgRating, count(*) as cnt FROM ratings GROUP BY movieId\")\n",
    "sqlDF2.createOrReplaceTempView(\"avgr\")\n",
    "val sqlDF3 = spark.sql(\"SELECT title, avgRating, cnt FROM avgr a, movies m WHERE a.movieId = m.movieId\")\n",
    "sqlDF3.explain\n",
    "sqlDF3.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b8706f-277f-43d6-94a1-85e8679cc1bc",
   "metadata": {},
   "source": [
    "Broadcasting can be enforced through hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fed25b-998d-4a53-afb2-92e0998a29ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val sqlDF = spark.sql(\"SELECT /*+  BROADCASTJOIN(m) */ m.title, avg(r.rating), count(*) FROM movies m, ratings r WHERE m.movieId=r.movieId GROUP BY m.title\")\n",
    "sqlDF.explain\n",
    "sqlDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856232b7-219f-4858-ab64-0e1589bf094c",
   "metadata": {},
   "source": [
    "Final version: broadcasting + pre-aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e902318e-eb1b-4d8c-9de5-d4884d8cbd7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val sqlDF2 = spark.sql(\"SELECT movieId, avg(rating) as avgRating, count(*) as cnt FROM ratings GROUP BY movieId\")\n",
    "sqlDF2.createOrReplaceTempView(\"avgr\")\n",
    "val sqlDF3 = spark.sql(\"SELECT /*+  BROADCASTJOIN(m) */ title, avgRating, cnt FROM avgr a, movies m WHERE a.movieId = m.movieId\")\n",
    "sqlDF3.show\n",
    "sqlDF3.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2539b2-eb7e-4197-be26-83ce7f366ed9",
   "metadata": {},
   "source": [
    "### 304-2 Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd627c-2210-48a1-8ebe-da364f25c92e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val path_ml_movies_parquet = \"s3a://\"+bucketname+\"/datasets/ml-movies-parquet\"\n",
    "val path_ml_ratings_parquet = \"s3a://\"+bucketname+\"/datasets/ml-ratings-parquet\"\n",
    "val path_ml_tags_parquet = \"s3a://\"+bucketname+\"/datasets/ml-tags-parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3407c90-95b1-4b35-a277-51f1199f8340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// Convert from RDD to Parquet\n",
    "rddMovies.toDF(\"movieId\",\"title\",\"genres\").coalesce(1).write.format(\"parquet\").mode(SaveMode.Overwrite).save(path_ml_movies_parquet)\n",
    "rddRatings.toDF(\"userid\",\"movieId\",\"rating\",\"year\").write.format(\"parquet\").mode(SaveMode.Overwrite).save(path_ml_ratings_parquet)\n",
    "rddTags.toDF(\"userId\",\"movieId\",\"tag\",\"year\").coalesce(1).write.format(\"parquet\").mode(SaveMode.Overwrite).save(path_ml_tags_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00750076-ca7d-45bd-9afc-1f4259bb5981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(\"s3a://\"+bucketname+\"/datasets/ml-movies-parquet\").createOrReplaceTempView(\"movies_pq\")\n",
    "spark.read.parquet(\"s3a://\"+bucketname+\"/datasets/ml-ratings-parquet\").createOrReplaceTempView(\"ratings_pq\")\n",
    "spark.read.parquet(\"s3a://\"+bucketname+\"/datasets/ml-tags-parquet\").createOrReplaceTempView(\"tags_pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56257c24-fed4-4cdd-9994-3ea40d240b05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val sqlDF = spark.sql(\"SELECT m.title, avg(r.rating), count(*) FROM movies_pq m, ratings_pq r WHERE m.movieId=r.movieId GROUP BY m.title\")\n",
    "sqlDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b538e99e-9a73-4b23-bd96-181ddf8c93af",
   "metadata": {},
   "source": [
    "### 304-3 SQL & Parquet on weather dataset\n",
    "\n",
    "Convert the full weather dataset to Parquet, then use SQL to join with the station dataset and calculate the average temperature by country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0341ac-f960-481b-8b2e-4291fb3f82fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// CHECK THE FILE NAMES\n",
    "val path_weather_full = \"s3a://\"+bucketname+\"/datasets/weather.txt\"\n",
    "val path_stations = \"s3a://\"+bucketname+\"/datasets/stations.csv\"\n",
    "\n",
    "val path_weather_full_parquet = \"s3a://\"+bucketname+\"/datasets/weather-full-parquet\"\n",
    "\n",
    "case class WeatherData(\n",
    "  usafwban:String,\n",
    "  year:String,\n",
    "  month:String,\n",
    "  day:String,\n",
    "  temperature:Double,\n",
    "  validTemperature:Boolean\n",
    ")\n",
    "\n",
    "object WeatherData {\n",
    "    def extract(row:String) = {\n",
    "        val usafwban = row.substring(4,15)\n",
    "        val year = row.substring(15,19)\n",
    "        val month = row.substring(19,21)\n",
    "        val day = row.substring(21,23)\n",
    "        val airTemperature = row.substring(87,92)\n",
    "        val airTemperatureQuality = row.charAt(92)\n",
    "\n",
    "        new WeatherData(usafwban,year,month,day,airTemperature.toInt/10,airTemperatureQuality == '1')\n",
    "    }\n",
    "}\n",
    "\n",
    "case class StationData(\n",
    "  usafwban:String,\n",
    "  name:String,\n",
    "  country:String,\n",
    "  state:String,\n",
    "  call:String,\n",
    "  latitude:Double,\n",
    "  longitude:Double,\n",
    "  elevation:Double,\n",
    "  date_begin:String,\n",
    "  date_end:String\n",
    ")\n",
    "\n",
    "object StationData {\n",
    "  def extract(row:String) = {\n",
    "    def getDouble(str:String) : Double = {\n",
    "      if (str.isEmpty)\n",
    "        return 0\n",
    "      else\n",
    "        return str.toDouble\n",
    "    }\n",
    "    val columns = row.split(\",\").map(_.replaceAll(\"\\\"\",\"\"))\n",
    "    val latitude = getDouble(columns(6))\n",
    "    val longitude = getDouble(columns(7))\n",
    "    val elevation = getDouble(columns(8))\n",
    "    new StationData(columns(0)+columns(1),columns(2),columns(3),columns(4),columns(5),latitude,longitude,elevation,columns(9),columns(10))\n",
    "  }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c4eee-10be-4076-86c2-008b0d7dfde6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// Convert from RDD to Parquet\n",
    "sc.textFile(path_weather_full).map(WeatherData.extract).\n",
    "    toDF(\"usafwban\",\"year\",\"month\",\"day\",\"airTemperature\",\"airTemperatureQuality\").\n",
    "    coalesce(20).\n",
    "    write.format(\"parquet\").mode(SaveMode.Overwrite).save(path_weather_full_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f49fda-4cf4-49b9-a7de-00ee7d42ef11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(path_weather_full_parquet).createOrReplaceTempView(\"weather_pq\")\n",
    "sc.textFile(path_stations).map(StationData.extract).toDF().createOrReplaceTempView(\"station_pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b5282-2a8c-46d6-bbc6-7c450a5cad4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val sqlDF = spark.sql(\"SELECT s.country, avg(w.airTemperature), count(*) FROM weather_pq w, station_pq s WHERE w.usafwban=s.usafwban GROUP BY s.country ORDER BY s.country\")\n",
    "sqlDF.explain\n",
    "sqlDF.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
