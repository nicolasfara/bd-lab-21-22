{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee",
      "metadata": {
        "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee"
      },
      "source": [
        "# 302 Spark optimizations\n",
        "\n",
        "The goal of this lab is to understand some of the optimization mechanisms of Spark.\n",
        "\n",
        "- [Spark programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
        "- [RDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html)\n",
        "- [PairRDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html)\n",
        "\n",
        "Let's start by setting the proper cluster configuration. In case of default setup (i.e., 2 [m5.xlarge](https://aws.amazon.com/it/ec2/instance-types/m5/) machines with 4 cores and 16 GB of RAM each):\n",
        "\n",
        "- 2 executors with 3 cores each (leave 1 for daemons; and there's also the AMP)\n",
        "- 8G of memory per executor (slide calculations would recommend 11G, but it exceeds YARN's default maximum allowed in this EMR cluster)\n",
        "\n",
        "Syntax details [here](https://aws.amazon.com/it/premiumsupport/knowledge-center/modify-spark-configuration-emr-notebook/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88ff8ad7-b73c-4a46-9e32-92b71d6a3479",
      "metadata": {
        "tags": [],
        "id": "88ff8ad7-b73c-4a46-9e32-92b71d6a3479"
      },
      "outputs": [],
      "source": [
        "%%configure -f\n",
        "{\"executorMemory\":\"8G\", \"numExecutors\":2, \"executorCores\":3, \"conf\": {\"spark.dynamicAllocation.enabled\": \"false\"}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c70c02bd-4c8f-4cc2-9a13-544da7c6544d",
      "metadata": {
        "tags": [],
        "id": "c70c02bd-4c8f-4cc2-9a13-544da7c6544d"
      },
      "outputs": [],
      "source": [
        "val bucketname = \"unibo-bd2122-egallinucci\"\n",
        "\n",
        "val path_weather = \"s3a://\"+bucketname+\"/datasets/weather-sample1.txt\"\n",
        "val path_weather_full = \"s3a://\"+bucketname+\"/datasets/weather-sample10.txt\"\n",
        "val path_stations = \"s3a://\"+bucketname+\"/datasets/weather-stations.csv\"\n",
        "\n",
        "\"SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/\" + sc.applicationId + \"/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7",
      "metadata": {
        "tags": [],
        "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7"
      },
      "outputs": [],
      "source": [
        "//Initialize the following objects to give some structure to the weather RDDs.\n",
        "\n",
        "case class WeatherData(\n",
        "  usaf:String,\n",
        "  wban:String,\n",
        "  year:String,\n",
        "  month:String,\n",
        "  day:String,\n",
        "  temperature:Double,\n",
        "  validTemperature:Boolean\n",
        ")\n",
        "\n",
        "object WeatherData {\n",
        "    def extract(row:String) = {\n",
        "        val usaf = row.substring(4,10)\n",
        "        val wban = row.substring(10,15)\n",
        "        val year = row.substring(15,19)\n",
        "        val month = row.substring(19,21)\n",
        "        val day = row.substring(21,23)\n",
        "        val airTemperature = row.substring(87,92)\n",
        "        val airTemperatureQuality = row.charAt(92)\n",
        "\n",
        "        new WeatherData(usaf,wban,year,month,day,airTemperature.toInt/10,airTemperatureQuality == '1')\n",
        "    }\n",
        "}\n",
        "\n",
        "case class StationData(\n",
        "  usaf:String,\n",
        "  wban:String,\n",
        "  name:String,\n",
        "  country:String,\n",
        "  state:String,\n",
        "  call:String,\n",
        "  latitude:Double,\n",
        "  longitude:Double,\n",
        "  elevation:Double,\n",
        "  date_begin:String,\n",
        "  date_end:String\n",
        ")\n",
        "\n",
        "object StationData {\n",
        "  def extract(row:String) = {\n",
        "    def getDouble(str:String) : Double = {\n",
        "      if (str.isEmpty)\n",
        "        return 0\n",
        "      else\n",
        "        return str.toDouble\n",
        "    }\n",
        "    val columns = row.split(\",\").map(_.replaceAll(\"\\\"\",\"\"))\n",
        "    val latitude = getDouble(columns(6))\n",
        "    val longitude = getDouble(columns(7))\n",
        "    val elevation = getDouble(columns(8))\n",
        "    new StationData(columns(0),columns(1),columns(2),columns(3),columns(4),columns(5),latitude,longitude,elevation,columns(9),columns(10))\n",
        "  }\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef4b49ee-6852-4025-9e55-3950ff937680",
      "metadata": {
        "id": "ef4b49ee-6852-4025-9e55-3950ff937680"
      },
      "source": [
        "## 302-1 Simple job optimization\n",
        "\n",
        "Optimize the two jobs (avg temperature and max temperature) by avoiding the repetition of the same computations and by defining a good number of partitions.\n",
        "\n",
        "Hints:\n",
        "- Verify your persisted data in the web UI\n",
        "- Use either ```repartition()``` or ```coalesce()``` to define the number of partitions\n",
        "  - ```repartition()``` shuffles all the data\n",
        "  - ```coalesce()``` minimizes data shuffling by exploiting the existing partitioning\n",
        "- Verify the execution plan of your RDDs with ```rdd.toDebugString``` (shell only) or on the web UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae20e128-aebc-4340-be2f-9da672fa81f8",
      "metadata": {
        "tags": [],
        "id": "ae20e128-aebc-4340-be2f-9da672fa81f8"
      },
      "outputs": [],
      "source": [
        "val rddWeather = sc.textFile(path_weather_full).map(WeatherData.extract)\n",
        "\n",
        "// Average temperature for every month\n",
        "rddWeather.\n",
        "    filter(_.temperature<999).\n",
        "    map(x => (x.month, x.temperature)).\n",
        "    aggregateByKey((0.0,0.0),1)((a,v)=>(a._1+v,a._2+1),(a1,a2)=>(a1._1+a2._1,a1._2+a2._2)).\n",
        "    map({case(k,v)=>(k,v._1/v._2)}).\n",
        "    collect()\n",
        "\n",
        "// Maximum temperature for every month\n",
        "rddWeather.\n",
        "    filter(_.temperature<999).\n",
        "    map(x => (x.month, x.temperature)).\n",
        "    reduceByKey((x,y)=>{if(x<y) y else x},1).\n",
        "    collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee3f9903-62cd-40a2-adc7-d821e924ea08",
      "metadata": {
        "tags": [],
        "id": "ee3f9903-62cd-40a2-adc7-d821e924ea08"
      },
      "source": [
        "### Solution\n",
        "\n",
        "Partitioning:\n",
        "- Consider 6 cores available in total\n",
        "- By default there are 42 partitions (i.e., 7 partitions per core), 32MB each\n",
        "  - No skewness --> no need to increase the number of partitions\n",
        "  - Task execution times are above 100-200ms -> no need to reduce the number of partitions\n",
        "  - Reducing to 12 partitions (i.e., 2 partitions per core, 64MB each) doesn't make much of a change in this example\n",
        "- After the shuffle, 42 partitions remain (due to the heritage mechanism)\n",
        "  - As we know that the aggregations greatly reduce the number of records, a fewer number of partitions can be specificed in the ```aggregateByKey``` and ```reduceByKey``` transformations\n",
        "\n",
        "Caching:\n",
        "- It is good to cache the RDD **after** the common transformations (including the coalescing) have been carried out\n",
        "- Then, remember to reference the cached RDD\n",
        "\n",
        "Check cached RDD in the Spark UI\n",
        "\n",
        "Coalesce vs repartition:\n",
        "- Coalesce aggregates partitions without shuffling; repartition forces a shuffle\n",
        "- None of them associates a partitioning criteria to the RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5841b168-df48-4e00-bd6a-6160b0838bc1",
      "metadata": {
        "tags": [],
        "id": "5841b168-df48-4e00-bd6a-6160b0838bc1"
      },
      "outputs": [],
      "source": [
        "val rddWeather = sc.textFile(path_weather_full).map(WeatherData.extract)\n",
        "\n",
        "val cachedRdd = rddWeather.filter(_.temperature<999).coalesce(12).map(x => (x.month, x.temperature)).cache()\n",
        "\n",
        "// Average temperature for every month\n",
        "cachedRdd.aggregateByKey((0.0,0.0))((a,v)=>(a._1+v,a._2+1), (a1,a2)=>(a1._1+a2._1,a1._2+a2._2)).map({case(k,v)=>(k,v._1/v._2)}).collect()\n",
        "// Maximum temperature for every month\n",
        "cachedRdd.reduceByKey((x,y)=>{if(x<y) y else x}).collect()\n",
        "\n",
        "rddWeather.coalesce(12).partitioner\n",
        "rddWeather.repartition(12).partitioner"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "377fbf30-f568-413c-9238-de139db23135",
      "metadata": {
        "id": "377fbf30-f568-413c-9238-de139db23135"
      },
      "source": [
        "## 302-2 RDD preparation\n",
        "\n",
        "Check the five possibilities to prepare the Station RDD for subsequent (multiple) processing and identify the best one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0",
      "metadata": {
        "tags": [],
        "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0"
      },
      "outputs": [],
      "source": [
        "import org.apache.spark.HashPartitioner\n",
        "val p = new HashPartitioner(8)\n",
        "\n",
        "val rddStation = sc.textFile(path_stations).map(StationData.extract)\n",
        "\n",
        "val rddS1 = rddStation.\n",
        "  keyBy(x => x.usaf + x.wban).\n",
        "  partitionBy(p).\n",
        "  cache().\n",
        "  map({case (k,v) => (k,(v.country,v.elevation))})\n",
        "val rddS2 = rddStation.\n",
        "  keyBy(x => x.usaf + x.wban).\n",
        "  map({case (k,v) => (k,(v.country,v.elevation))}).\n",
        "  cache().\n",
        "  partitionBy(p)\n",
        "val rddS3 = rddStation.\n",
        "  keyBy(x => x.usaf + x.wban).\n",
        "  partitionBy(p).\n",
        "  map({case (k,v) => (k,(v.country,v.elevation))}).\n",
        "  cache()\n",
        "val rddS4 = rddStation.\n",
        "  keyBy(x => x.usaf + x.wban).\n",
        "  map({case (k,v) => (k,(v.country,v.elevation))}).\n",
        "  partitionBy(p).\n",
        "  cache()\n",
        "val rddS5 = rddStation.\n",
        "  map(x => (x.usaf + x.wban, (x.country,x.elevation))).\n",
        "  partitionBy(p).\n",
        "  cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e97789d-9b91-4d68-a1df-f84e7dccc759",
      "metadata": {
        "id": "0e97789d-9b91-4d68-a1df-f84e7dccc759"
      },
      "source": [
        "### Solution\n",
        "\n",
        "- keyBy() and map() break the partitioning, thus they must be issued before partitionBy()\n",
        "- Anything that happens after cache() is not saved and must be recomputed each time;\n",
        "  thus, it is good practice to cache() as later as possible\n",
        "- rddS4 and rddS5 are the best options (the latter being less verbose)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed",
      "metadata": {
        "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed"
      },
      "source": [
        "## 302-3 Joining RDDs\n",
        "\n",
        "Define the join between rddWeather and rddStation and compute:\n",
        "- The maximum temperature for every city\n",
        "- The maximum temperature for every city in the UK: \n",
        "  - ```StationData.country == \"UK\"```\n",
        "- Sort the results by descending temperature\n",
        "  - ```map({case(k,v)=>(v,k)})``` to invert key with value and vice versa\n",
        "\n",
        "Hints & considerations:\n",
        "- Keep only temperature values <999\n",
        "- Join syntax: ```rdd1.join(rdd2)```\n",
        "  - Both RDDs should be structured as key-value RDDs with the same key: usaf + wban\n",
        "- Consider partitioning and caching to optimize the join\n",
        "  - Careful: it is not enough for the two RDDs to have the same number of partitions; they must have the same partitioner!\n",
        "- Verify the execution plan of the join in the web UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e32bce1c-e062-478c-9c08-d9017c26a6ca",
      "metadata": {
        "id": "e32bce1c-e062-478c-9c08-d9017c26a6ca"
      },
      "outputs": [],
      "source": [
        "val rddWeather = sc.textFile(path_weather).map(WeatherData.extract)\n",
        "val rddStation = sc.textFile(path_stations).map(StationData.extract)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8ae6820-fd26-4be3-aa63-683823a6b0a3",
      "metadata": {
        "id": "e8ae6820-fd26-4be3-aa63-683823a6b0a3"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fbfb6fb-1c5d-450a-b0d3-42c2be226fad",
      "metadata": {
        "tags": [],
        "id": "7fbfb6fb-1c5d-450a-b0d3-42c2be226fad"
      },
      "outputs": [],
      "source": [
        "import org.apache.spark.HashPartitioner\n",
        "\n",
        "val p = new HashPartitioner(14)\n",
        "\n",
        "val rddWeather = sc.textFile(path_weather).map(WeatherData.extract)\n",
        "val rddStation = sc.textFile(path_stations).map(StationData.extract)\n",
        "\n",
        "val rddS = rddStation.keyBy(x => x.usaf + x.wban).partitionBy(p)\n",
        "val rddW = rddWeather.filter(_.temperature<999).keyBy(x => x.usaf + x.wban).partitionBy(p)\n",
        "\n",
        "val rddJoin = rddW.join(rddS).cache()\n",
        "rddJoin.toDebugString\n",
        "\n",
        "rddJoin.map({case(k,v)=>(v._2.name,v._1.temperature)}).reduceByKey((x,y)=>{if(x<y) y else x}).collect()\n",
        "rddJoin.filter(_._2._2.country==\"UK\").map({case(k,v)=>(v._2.name,v._1.temperature)}).reduceByKey((x,y)=>{if(x<y) y else x}).collect()\n",
        "rddJoin.filter(_._2._2.country==\"UK\").map({case(k,v)=>(v._2.name,v._1.temperature)}).reduceByKey((x,y)=>{if(x<y) y else x}).map({case(k,v)=>(v,k)}).sortByKey(false).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882",
      "metadata": {
        "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882"
      },
      "source": [
        "## 302-4 Memory occupation\n",
        "\n",
        "Use Spark's web UI to verify the space occupied by the provided RDDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af3068b3-f2aa-4d13-812b-7d0461a35390",
      "metadata": {
        "tags": [],
        "id": "af3068b3-f2aa-4d13-812b-7d0461a35390"
      },
      "outputs": [],
      "source": [
        "import org.apache.spark.storage.StorageLevel._\n",
        "val rddWeather = sc.textFile(path_weather_full).map(WeatherData.extract)\n",
        "\n",
        "sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
        "\n",
        "val memRdd = rddWeather.repartition(14).cache()\n",
        "val memSerRdd = memRdd.map(x=>x).persist(MEMORY_ONLY_SER)\n",
        "val diskRdd = memRdd.map(x=>x).persist(DISK_ONLY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce473ab4-b676-4ae3-b7e6-a4c61f1b3c3e",
      "metadata": {
        "id": "ce473ab4-b676-4ae3-b7e6-a4c61f1b3c3e"
      },
      "source": [
        "### Solution\n",
        "\n",
        "- Collecting/Saving to file is required to trigger the (lazy) evaluation\n",
        "- memSerRdd's size will be less than half of memRdd's\n",
        "- diskRdd's size will be approximately the same as memSerRdd (output to disk is always serialized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "404753f7-1dd6-4997-a0af-9d43f3e9c4ae",
      "metadata": {
        "tags": [],
        "id": "404753f7-1dd6-4997-a0af-9d43f3e9c4ae"
      },
      "outputs": [],
      "source": [
        "memRdd.saveAsTextFile(\"s3a://\"+bucketname+\"/tmp/rddW-m\")\n",
        "memSerRdd.saveAsTextFile(\"s3a://\"+bucketname+\"/tmp/rddW-md\")\n",
        "diskRdd.saveAsTextFile(\"s3a://\"+bucketname+\"/tmp/rddW-d\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4c7bc50-bb59-4e70-8955-8a44d7de774d",
      "metadata": {
        "id": "f4c7bc50-bb59-4e70-8955-8a44d7de774d"
      },
      "source": [
        "## 302-5 Evaluating different join methods\n",
        "\n",
        "Consider the following scenario:\n",
        "- We have a disposable RDD of Weather data (i.e., it is used only once): ```rddW```\n",
        "- And we have an RDD of Station data that is used many times: ```rddS```\n",
        "- Both RDDs are cached (```collect()```is called to enforce caching)\n",
        "\n",
        "We want to join the two RDDS. Which option is best?\n",
        "- Simply join the two RDDs\n",
        "- Enforce on ```rddW1``` the same partitioner of ```rddS``` (and then join)\n",
        "- Exploit broadcast variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31d77122-8bdd-4784-a86e-f42f2da06759",
      "metadata": {
        "tags": [],
        "id": "31d77122-8bdd-4784-a86e-f42f2da06759"
      },
      "outputs": [],
      "source": [
        "import org.apache.spark.HashPartitioner\n",
        "\n",
        "val rddWeather = sc.textFile(path_weather_full).map(WeatherData.extract)\n",
        "val rddStation = sc.textFile(path_stations).map(StationData.extract)\n",
        "\n",
        "val p = new HashPartitioner(14)\n",
        "\n",
        "val rddW = rddWeather.\n",
        "  filter(_.temperature<999).\n",
        "  keyBy(x => x.usaf + x.wban).\n",
        "  cache()\n",
        "val rddS = rddStation.\n",
        "  keyBy(x => x.usaf + x.wban).\n",
        "  partitionBy(p).\n",
        "  cache()\n",
        "\n",
        "// Collect to enforce caching\n",
        "rddW.saveAsTextFile(\"s3a://\"+bucketname+\"/tmp/rddW\")\n",
        "rddS.saveAsTextFile(\"s3a://\"+bucketname+\"/tmp/rddS\")\n",
        "\n",
        "// Is it better to simply join the two RDDs..\n",
        "rddW.\n",
        "  join(rddS).\n",
        "  map({case(k,v)=>(v._2.name,v._1.temperature)}).\n",
        "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
        "  collect\n",
        "\n",
        "// ..to enforce on rddW1 the same partitioner of rddS..\n",
        "rddW.\n",
        "  partitionBy(p).\n",
        "  join(rddS).\n",
        "  map({case(k,v)=>(v._2.name,v._1.temperature)}).\n",
        "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
        "  collect()\n",
        "\n",
        "// // ..or to exploit broadcast variables?\n",
        "val bRddS = sc.broadcast(rddS.map(x => (x._1, x._2.name)).collectAsMap())\n",
        "val rddJ = rddW.\n",
        "  map({case (k,v) => (bRddS.value.get(k),v.temperature)}).\n",
        "  filter(_._1!=None)\n",
        "rddJ.\n",
        "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
        "  collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52356053-f554-4d8a-a1cb-9ca0e51bbc6d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T19:34:51.195508Z",
          "iopub.status.busy": "2022-03-13T19:34:51.195245Z",
          "iopub.status.idle": "2022-03-13T19:34:51.642761Z",
          "shell.execute_reply": "2022-03-13T19:34:51.641847Z",
          "shell.execute_reply.started": "2022-03-13T19:34:51.195481Z"
        },
        "tags": [],
        "id": "52356053-f554-4d8a-a1cb-9ca0e51bbc6d"
      },
      "source": [
        "### Solution\n",
        "\n",
        "The first and second options both take approximately the same time\n",
        "  - The first one shuffles less data\n",
        "  - The second one allows skipping unnecessary partitions from rddS\n",
        "  \n",
        "The third option is faster\n",
        "  - rddW data is not shuffled\n",
        "  - The downside is the increased space occupation in the executors' memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9cc81c0-1425-4ef9-8a19-a7edca031c33",
      "metadata": {
        "id": "e9cc81c0-1425-4ef9-8a19-a7edca031c33"
      },
      "source": [
        "## 302-6 Optimizing Exercise 3\n",
        "\n",
        "Start from the result of Exercise 3; is there a more efficient way to compute the same result?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47748353-fb4b-432f-af79-d1136453b956",
      "metadata": {
        "tags": [],
        "id": "47748353-fb4b-432f-af79-d1136453b956"
      },
      "outputs": [],
      "source": [
        "val rddWeather = sc.textFile(path_weather_full).map(WeatherData.extract)\n",
        "val rddStation = sc.textFile(path_stations).map(StationData.extract)\n",
        "\n",
        "val rddW = rddWeather.filter(_.temperature<999).keyBy(x => x.usaf + x.wban).cache()\n",
        "val rddS = rddStation.keyBy(x => x.usaf + x.wban).cache()\n",
        "\n",
        "val rdd6a = rddW.\n",
        "  join(rddS).\n",
        "  filter(_._2._2.country==\"UK\").\n",
        "  map({case(k,v)=>(v._2.name,v._1.temperature)}).\n",
        "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
        "  map({case(k,v)=>(v,k)}).\n",
        "  sortByKey(false).\n",
        "  collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61f8cc89-9a7f-4c79-94c1-5fdcd72618e8",
      "metadata": {
        "id": "61f8cc89-9a7f-4c79-94c1-5fdcd72618e8"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67f448cc-efc7-4793-a3a2-4a19e0e6fc15",
      "metadata": {
        "tags": [],
        "id": "67f448cc-efc7-4793-a3a2-4a19e0e6fc15"
      },
      "outputs": [],
      "source": [
        "// First version\n",
        "val rdd6a = rddW.\n",
        "  join(rddS).filter(_._2._2.country==\"UK\").\n",
        "  map({case(k,v)=>(v._2.name,v._1.temperature)}).\n",
        "  reduceByKey((x,y)=>{if(x<y) y else x}).map({case(k,v)=>(v,k)}).\n",
        "  sortByKey(false).\n",
        "  collect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60ac41c6-5b59-40ae-8492-f113ed9bdb8f",
      "metadata": {
        "tags": [],
        "id": "60ac41c6-5b59-40ae-8492-f113ed9bdb8f"
      },
      "outputs": [],
      "source": [
        "// Pushing down filters is always a good move\n",
        "val rdd6b = rddW.\n",
        "  join(rddS.filter(_._2.country==\"UK\")).\n",
        "  map({case(k,v)=>(v._2.name,v._1.temperature)}).\n",
        "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
        "  map({case(k,v)=>(v,k)}).\n",
        "  sortByKey(false).\n",
        "  collect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6083c12-1ec5-414c-82da-0a1092973851",
      "metadata": {
        "tags": [],
        "id": "c6083c12-1ec5-414c-82da-0a1092973851"
      },
      "outputs": [],
      "source": [
        "// Pushing down aggregations is even better\n",
        "val rdd6c = rddW.\n",
        "  map({case(k,v)=>(k,v.temperature)}).\n",
        "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
        "  join(rddS).filter(_._2._2.country==\"UK\").\n",
        "  map({case(k,v)=>(v._1,v._2.name)}).\n",
        "  sortByKey(false).\n",
        "  collect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8381655b-2270-4340-ba72-69c3f3092a4a",
      "metadata": {
        "tags": [],
        "id": "8381655b-2270-4340-ba72-69c3f3092a4a"
      },
      "outputs": [],
      "source": [
        "// Best option: pushing down both filters and aggregations\n",
        "val rdd6d = rddW.\n",
        "  map({case(k,v)=>(k,v.temperature)}).\n",
        "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
        "  join(rddS.filter(_._2.country==\"UK\")).\n",
        "  map({case(k,v)=>(v._1,v._2.name)}).\n",
        "  sortByKey(false).\n",
        "  collect"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Spark",
      "language": "",
      "name": "sparkkernel"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "colab": {
      "name": "302-solutions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}